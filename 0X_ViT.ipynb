{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Bonus Notebook\n",
    "\n",
    "This is a bonus notebook that demonstrates how to use the Vision Transformer (ViT) model for image classification. The dataset used is the [Bees vs Wasps dataset](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp) from Kaggle. The goal, as in notebooks 1 and 1.2 is to classify the images into one of the four classes.\n",
    "\n",
    "As before, the dataset was found on Kaggle. [Check out the dataset information](https://www.kaggle.com/datasets/jerzydziewierz/bee-vs-wasp)\n",
    "\n",
    "<img src=\"images/vit_cover_image.png\" \n",
    "        alt=\"Image of an insect broken into transformer tokens\" \n",
    "        width=\"1000\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "We will code our own ViT model from scratch using PyTorch. We will also use the `torchvision` library to load the dataset and preprocess the images. Vision Transformers (ViTs) are a newer class of models that have shown great promise in image classification tasks. They are based on the Transformer architecture, which was originally designed for natural language processing tasks. ViTs have shown to be competitive with CNNs on image classification tasks, and have the added advantage of being able to capture long-range dependencies in the data. The downsides of ViTs are that they can be computationally expensive to train, and require larger amounts of data to perform well versus CNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing supporting libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for the dataset\n",
    "import numpy as np # NumPy\n",
    "from PIL import Image # Python Imaging Library\n",
    "import pandas as pd # Pandas\n",
    "from sklearn.model_selection import train_test_split # Train-Test Split\n",
    "from sklearn.preprocessing import StandardScaler # Standard Scaler\n",
    "from sklearn.datasets import fetch_openml # Fetch OpenML\n",
    "from sklearn.metrics import accuracy_score # Accuracy Score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Importing the necessary libraries for the visualization\n",
    "import matplotlib.pyplot as plt # Matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Importing the necessary libraries for ViT\n",
    "import torch # PyTorch\n",
    "import torch.nn as nn # Neural Network Module\n",
    "import torch.nn.functional as F # Functional Module that contains activation functions\n",
    "import torchvision # TorchVision\n",
    "from tqdm import tqdm # TQDM for displaying progress bar\n",
    "from einops import rearrange # Rearrange function that rearranges the dimensions of the input tensor\n",
    "from einops.layers.torch import Rearrange # Rearrange Layer that rearranges the dimensions of the input tensor\n",
    "from torch import einsum # Einsum function that performs a contraction on the input tensor, similar to a Convolutional Layer\n",
    "from torchvision import transforms # Transforms that are used for data augmentation\n",
    "# from torchsummary import summary # Summary function that displays the architecture of the model\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "As we did in Notebook 1, we will have to load the dataset. If you haven't done so yet, you can use the code in the cell below to download and extract the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_file(url=\"https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz?dl=1\", filename=\"bee_vs_wasp.tar.gz\"):\n",
    "\n",
    "    # Download the file using requests\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Create a file object and write the response content in chunks\n",
    "    with open(filename, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    # Wait for the file to finish downloading\n",
    "    while not os.path.exists(filename):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Print a success message\n",
    "    print(f\"Downloaded {filename} successfully.\")\n",
    "\n",
    "def extract_file(filename, data_folder):\n",
    "    # Check if the file is a tar file\n",
    "    if tarfile.is_tarfile(filename):\n",
    "        # Open the tar file\n",
    "        tar = tarfile.open(filename, \"r:gz\")\n",
    "        # Extract all the files to the data folder\n",
    "        tar.extractall(data_folder)\n",
    "        # Close the tar file\n",
    "        tar.close()\n",
    "        # Print a success message\n",
    "        print(f\"Extracted {filename} to {data_folder} successfully.\")\n",
    "    else:\n",
    "        # Print an error message\n",
    "        print(f\"{filename} is not a valid tar file.\")\n",
    "    \n",
    "def manage_data(folder_name='bee_vs_wasp'):\n",
    "    '''Try to find the data for the exercise and return the path'''\n",
    "    \n",
    "    # Check common paths of where the data might be on different systems\n",
    "    likely_paths= [os.path.normpath(f'/blue/practicum-ai/share/data/{folder_name}'),\n",
    "                   os.path.normpath(f'/project/scinet_workshop2/data/{folder_name}'),\n",
    "                   os.path.join('data', folder_name),\n",
    "                   os.path.normpath(folder_name)]\n",
    "    \n",
    "    for path in likely_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f'Found data at {path}.')\n",
    "            return path\n",
    "\n",
    "    answer = input(f'Could not find data in the common locations. Do you know the path? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "        path = os.path.join(os.path.normpath(input('Please enter the path to the data folder: ')),folder_name)\n",
    "        if os.path.exists(path):\n",
    "            print(f'Thanks! Found your data at {path}.')\n",
    "            return path\n",
    "        else:\n",
    "            print(f'Sorry, that path does not exist.')\n",
    "    \n",
    "    answer = input('Do you want to download the data? (yes/no): ')\n",
    "\n",
    "    if answer.lower() == 'yes':\n",
    "\n",
    "        ''' Check and see if the downloaded data is inside the .gitignore file, and adds them to the list of files to ignore if not. \n",
    "        This is to prevent the data from being uploaded to the repository, as the files are too large for GitHub.'''\n",
    "        \n",
    "        if os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'r') as f:\n",
    "                ignore = f.read().split('\\n')\n",
    "        # If the .gitignore file does not exist, create a new one\n",
    "        elif not os.path.exists('.gitignore'):\n",
    "            with open('.gitignore', 'w') as f:\n",
    "                f.write('')\n",
    "            ignore = []\n",
    "        else:\n",
    "            ignore = []\n",
    "\n",
    "        # Check if the .gz file is in the ignore list\n",
    "        if 'bee_vs_wasp.tar.gz' not in ignore:\n",
    "            ignore.append('bee_vs_wasp.tar.gz')\n",
    "            \n",
    "        # Check if the data/ folder is in the ignore list\n",
    "        if 'data/' not in ignore:\n",
    "            ignore.append('data/')\n",
    "\n",
    "        # Write the updated ignore list back to the .gitignore file\n",
    "        with open('.gitignore', 'w') as f:\n",
    "            f.write('\\n'.join(ignore))\n",
    "\n",
    "        print(\"Updated .gitignore file.\")\n",
    "        print('Downloading data, this may take a minute.')\n",
    "        download_file()\n",
    "        print('Data downloaded, unpacking')\n",
    "        extract_file(\"bee_vs_wasp.tar.gz\", \"data\")\n",
    "        print('Data downloaded and unpacked. Now available at data/bee_vs_wasp.')\n",
    "        return os.path.normpath('data/bee_vs_wasp')   \n",
    "\n",
    "    print('Sorry, I cannot find the data. Please download it manually from https://www.dropbox.com/s/x70hm8mxqhe7fa6/bee_vs_wasp.tar.gz and unpack it to the data folder.')      \n",
    "\n",
    "\n",
    "data_path = manage_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains.\n",
    "\n",
    "!!! Make Notes Here !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_display_data(data_path):\n",
    "    '''Check the contents of the data folder'''\n",
    "    \n",
    "    print(\"***********************************************************************\")\n",
    "    # Check the contents of the data folder\n",
    "    data_folder = pathlib.Path(data_path)\n",
    "    # Check if the data folder exists\n",
    "    if data_folder.exists():\n",
    "        # Create a list of the contents of the data folder\n",
    "        data_folder_contents = list(data_folder.glob('*/*'))\n",
    "        # Create a list of the classes in the data folder\n",
    "        classes = [item.name for item in data_folder.glob('*') if item.is_dir()]\n",
    "        # Print the classes in the data folder\n",
    "        print(f'The data folder contains the following classes: {classes}')\n",
    "        # Create a dictionary to store the class distribution\n",
    "        class_distribution = {}\n",
    "        # Create a list to store the paths of the images\n",
    "        image_paths = []\n",
    "        # Create a list to store the labels of the images\n",
    "        labels = []\n",
    "        # Loop through the classes in the data folder\n",
    "        for class_name in classes:\n",
    "            # Create a list of the contents of the class folder\n",
    "            class_folder_contents = list(data_folder.glob(f'{class_name}/*'))\n",
    "            # Filter out any directories that start with \".ipynb_checkpoints\"\n",
    "            class_folder_contents = [item for item in class_folder_contents if not item.name.startswith('.ipynb_checkpoints')]\n",
    "            # Print the number of images in the class folder\n",
    "            print(f'The class {class_name} contains {len(class_folder_contents)} images.')\n",
    "            # Update the class distribution dictionary with the class name and the number of images\n",
    "            class_distribution[class_name] = len(class_folder_contents)\n",
    "            # Add the paths of the images to the image paths list\n",
    "            image_paths.extend(class_folder_contents[:4])\n",
    "            # Add the labels of the images to the labels list\n",
    "            labels.extend([class_name]*4)\n",
    "        # Create a figure to display the images\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(20, 10))\n",
    "        # Loop through the image paths and labels\n",
    "        print(\"***********************************************************************\")\n",
    "        for i, (image_path, label) in enumerate(zip(image_paths, labels)):\n",
    "            # Load the image\n",
    "            image = Image.open(image_path)\n",
    "            # Display the image\n",
    "            axes[i//4, i%4].imshow(image)\n",
    "            axes[i//4, i%4].set_title(label)\n",
    "            axes[i//4, i%4].axis('off')\n",
    "        # Show the figure\n",
    "        plt.show()\n",
    "\n",
    "        # Create train and validation dataframes\n",
    "        train_data = []\n",
    "        for class_name in classes:\n",
    "            class_folder_contents = list(data_folder.glob(f'{class_name}/*'))\n",
    "            class_folder_contents = [item for item in class_folder_contents if not item.name.startswith('.ipynb_checkpoints')]\n",
    "            for item in class_folder_contents:\n",
    "                train_data.append((item, class_name))\n",
    "        train_df = pd.DataFrame(train_data, columns=['image', 'class'])\n",
    "        train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['class'], random_state=42)\n",
    "        print(f'The training dataset contains {len(train_df)} images.')\n",
    "        print(f'The validation dataset contains {len(val_df)} images.')\n",
    "\n",
    "        # Create a figure to display the class distribution ordered from highest to lowest\n",
    "        fig, ax = plt.subplots() \n",
    "        # Sort the class distribution dictionary by the number of images\n",
    "        class_distribution = dict(sorted(class_distribution.items(), key=lambda item: item[1], reverse=True))\n",
    "        # Plot the class distribution as a histogram\n",
    "        ax.bar(class_distribution.keys(), class_distribution.values())\n",
    "        # Set the title of the plot\n",
    "        ax.set_title('Class Distribution')\n",
    "        # Set the x-label of the plot\n",
    "        ax.set_xlabel('Class')\n",
    "\n",
    "        # Set the y-label of the plot\n",
    "        ax.set_ylabel('Number of Images')\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Print an error message if the data folder does not exist\n",
    "        print(f'The data folder {data_folder} does not exist.')\n",
    "\n",
    "    return train_df, val_df\n",
    "\n",
    "train_df, val_df = prep_display_data(data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of wasps! At least compared to the number of bees and other classes. We will have to see how ViTs perform with unbalanced datasets as compared to the CNNs in notebooks 1 and 1.2.\n",
    "\n",
    "## 4. Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up our ViT model architecture\n",
    "\n",
    "# Define the Patch Embedding Layer (Convolutional Layer)\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size):\n",
    "        super(PatchEmbedding, self).__init__() # Call the constructor of the parent class\n",
    "        self.patch_size = patch_size # Set the patch size\n",
    "        # Define the projection layer that converts the input image into a sequence of patches\n",
    "        self.projection = nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Rearrange the input tensor x to have a new shape (B, E, H, W) \n",
    "        where E is the number of patches and (H, W) is the shape of the patches.'''\n",
    "        x = self.projection(x)\n",
    "        x = rearrange(x, 'b e (h) (w) -> b (h w) e', h=x.shape[2], w=x.shape[3])\n",
    "        return x\n",
    "    \n",
    "# Define the Multi-Head Self-Attention Layer\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dropout=0.0): # Constructor\n",
    "        super(MultiHeadAttention, self).__init__()      # Call the constructor of the parent class\n",
    "        self.emb_size = emb_size                        # Set the embedding size\n",
    "        self.num_heads = num_heads                      # Set the number of heads for the multi-head attention\n",
    "        self.keys = nn.Linear(emb_size, emb_size)       # Linear Layer for the keys in the scaled dot-product attention\n",
    "        self.queries = nn.Linear(emb_size, emb_size)    # Linear Layer for the queries in the scaled dot-product attention\n",
    "        self.values = nn.Linear(emb_size, emb_size)     # Linear Layer for the values in the scaled dot-product attention\n",
    "        self.fc_out = nn.Linear(emb_size, emb_size)     # Linear Layer for the output of the multi-head attention\n",
    "        self.dropout = nn.Dropout(dropout)              # Dropout Layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split the input tensor x into num_heads pieces\n",
    "        queries = rearrange(self.queries(x), 'b n (h d) -> b h n d', h=self.num_heads) # Rearrange the queries\n",
    "        keys = rearrange(self.keys(x), 'b n (h d) -> b h n d', h=self.num_heads)      # Rearrange the keys\n",
    "        values = rearrange(self.values(x), 'b n (h d) -> b h n d', h=self.num_heads) # Rearrange the values\n",
    "        \n",
    "        # Perform a scaled dot-product attention with the queries, keys, and values\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) / (self.emb_size ** 0.5)\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "        attention = self.dropout(attention)\n",
    "        \n",
    "        # Apply the attention to the values and concatenate the heads back together\n",
    "        out = torch.einsum('bhal, bhlv -> bhav', attention, values)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "    \n",
    "# Define the Multi-Layer Perceptron (MLP) Layer for the Transformer Block\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, emb_size, mlp_hidden_dim, dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(emb_size, mlp_hidden_dim) # Linear Layer for the first hidden layer\n",
    "        self.fc2 = nn.Linear(mlp_hidden_dim, emb_size) # Linear Layer for the second hidden layer\n",
    "        self.dropout = nn.Dropout(dropout)             # Dropout Layer for regularization\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x)) # Apply the GELU activation function to the first hidden layer\n",
    "        x = self.dropout(x)    # Apply dropout to the first hidden layer\n",
    "        x = self.fc2(x)       # Apply the second hidden layer\n",
    "        return x\n",
    "\n",
    "# Define the Transformer Block that consists of a Multi-Head Self-Attention Layer and a Multi-Layer Perceptron (MLP) Layer\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(emb_size, num_heads, dropout)\n",
    "        self.norm1 = nn.LayerNorm(emb_size)\n",
    "        self.mlp = MLP(emb_size, mlp_hidden_dim, dropout)\n",
    "        self.norm2 = nn.LayerNorm(emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.dropout(self.attention(self.norm1(x))) # Add the residual connection and apply dropout \n",
    "        x = x + self.dropout(self.mlp(self.norm2(x)))       # Add the residual connection and apply dropout \n",
    "        return x\n",
    "    \n",
    "# Define the Vision Transformer (ViT) Model\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout=0.0):\n",
    "        super(ViT, self).__init__()\n",
    "        num_patches = (img_size // patch_size) ** 2 # Calculate the number of patches in the image\n",
    "        self.patch_embedding = PatchEmbedding(in_channels, patch_size, emb_size)    # Patch Embedding Layer\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, emb_size))                  # Learnable parameter for the class token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, emb_size)) # Learnable parameter for the positional embeddings\n",
    "        # Create the Transformer Encoder with num_layers of Transformer Blocks\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            Transformer(emb_size, num_heads, mlp_hidden_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc = nn.Linear(emb_size, num_classes) # Linear Layer for the final classification\n",
    "        self.dropout = nn.Dropout(dropout)        # Dropout Layer for regularization\n",
    "\n",
    "    def forward(self, x):                   # Forward pass of the model\n",
    "        x = self.patch_embedding(x)       # Apply the Patch Embedding Layer\n",
    "        b, n, _ = x.shape                 # Get the batch size and number of patches\n",
    "        cls_tokens = self.cls_token.expand(b, -1, -1) # Expand the learnable class token parameter for the batch\n",
    "        x = torch.cat([cls_tokens, x], dim=1)       # Concatenate the class token with the patches\n",
    "        x += self.pos_embedding                  # Add the positional embeddings\n",
    "        x = self.dropout(x)                     # Apply dropout to the input\n",
    "        x = self.transformer(x)               # Apply the Transformer Encoder\n",
    "        x = x.mean(dim=1)                    # Average pool over the patches\n",
    "        x = self.fc(x)                      # Apply the final classification layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a ViT model instance\n",
    "\n",
    "# Define the model hyperparameters\n",
    "in_channels = 3 # Number of input channels (RGB images)\n",
    "patch_size = 16 # Patch size of the token means the size of the image that will be converted to a token\n",
    "emb_size = 768 # Embedding size of the token means the size of the token after the linear transformation\n",
    "img_size = 128 # Image size of the input image\n",
    "num_classes = 6 # Number of classes in the dataset\n",
    "num_layers = 12 # Number of layers in the Transformer Encoder\n",
    "num_heads = 12 # Number of heads in the Multi-Head Self-Attention Layer\n",
    "mlp_hidden_dim = 3072 # Hidden dimension of the Multi-Layer Perceptron (MLP) Layer\n",
    "dropout = 0.1 # Dropout rate for regularization in the model (0.1 means 10% of the neurons will be dropped out during training)\n",
    "\n",
    "\n",
    "model = ViT(in_channels, patch_size, emb_size, img_size, num_classes, num_layers, num_heads, mlp_hidden_dim, dropout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 What is a Vision Transformer \"seeing\"?\n",
    "The code above probably looks pretty confusing. Let's break down what's happening in the Vision Transformer model step by step:\n",
    "##### A. Image Patching:\n",
    "Divide the input image into a grid of non-overlapping patches (e.g., usually 16x16 pixels each.) This transforms the 2D image into a sequence of smaller 2D patches, treating each patch as a \"token\" similar to a word in NLP.\n",
    "##### B. Linear Embedding:\n",
    "Flatten each image patch into a 1D vector and linearly project it to a higher-dimensional space. This creates a fixed-size representation (embedding) for each patch.\n",
    "##### C. Position Embedding:\n",
    "Add learnable positional embeddings to each patch embedding. Injects information about the position of each patch in the original image, preserving the spatial structure.\n",
    "##### D. Transformer Encoder:\n",
    "Pass the sequence of patch embeddings through multiple layers of the Transformer encoder.\n",
    "Each encoder layer consists of:\n",
    "\n",
    "**Multi-Head Self-Attention**: Computes attention scores for each patch with every other patch, capturing relationships between patches.\n",
    "\n",
    "**Feed-Forward Network**: Applies a fully connected neural network to each patch embedding.\n",
    "\n",
    "**Layer Normalization and Residual Connections**: Ensure stability and gradient flow.\n",
    "This allows the Transformer to learn complex interactions and dependencies between patches by attending to all patches simultaneously.\n",
    "\n",
    "##### E. Classification Token:\n",
    "Introduces a learnable token at the beginning of the patch sequence, aggregating information from all patches. Uses the final representation of the token for classification.\n",
    "##### F. Output Layer:\n",
    "Passes the final token representation through a linear layer to obtain class logits.\n",
    "The purpose is to predict the class label for the entire image.\n",
    "\n",
    "### Capturing Long-Range Dependencies\n",
    "\n",
    "**Self-Attention Mechanism**: The self-attention mechanism in each Transformer encoder layer computes attention scores between every pair of patches. This means that each patch can directly attend to any other patch in the image, regardless of their spatial distance.\n",
    "\n",
    "**Global Context**: Since attention is applied globally, the model can capture dependencies between distant patches, allowing it to understand the entire image context.\n",
    "\n",
    "**Flexible Interactions**: Unlike CNNs, which rely on local convolutions and pooling to gradually increase the receptive field, the Transformer can model interactions between any patches in a single layer, making it inherently capable of capturing long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show a random image from the dataset\n",
    "image_path = random.choice(train_df['image'])\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Show the image broken up into patches\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "image_tensor = transform(image).unsqueeze(0)\n",
    "patch_size = 16\n",
    "patches = rearrange(image_tensor, 'b c (h s1) (w s2) -> b (h w) c s1 s2', s1=patch_size, s2=patch_size) # Rearrange the dimensions of the input tensor\n",
    "n_patches = patches.shape[1] # Get the number of patches\n",
    "n_patches_side = int(np.sqrt(n_patches))\n",
    "fig, axes = plt.subplots(n_patches_side, n_patches_side, figsize=(10, 10))\n",
    "\n",
    "for i in range(n_patches):\n",
    "    patch = patches[0, i].permute(1, 2, 0)\n",
    "    axes[i // n_patches_side, i % n_patches_side].imshow(patch)\n",
    "    axes[i // n_patches_side, i % n_patches_side].axis('off')\n",
    "\n",
    "# Show the image patches\n",
    "plt.imshow(patch)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "patch = patches[0, i].unsqueeze(0).permute(0, 2, 3, 1).squeeze(0)\n",
    "patches = patches[:, torch.randperm(patches.size(1))]\n",
    "\n",
    "# Show the image patches plotted randomly in a single row\n",
    "fig, ax = plt.subplots(1, n_patches, figsize=(20, 20))\n",
    "for i in range(n_patches):\n",
    "    patch = patches[0, i].permute(1, 2, 0)\n",
    "    ax[i].imshow(patch)\n",
    "    ax[i].axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # Cross-Entropy Loss, other options include NLLLoss and KLDivLoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001) # Adam Optimizer with a learning rate of 0.0001, other options include SGD and RMSprop\n",
    "\n",
    "# Define the training function\n",
    "def train(model, data_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0              # Initialize the running loss to 0\n",
    "    train_correct = 0               # Initialize the number of correctly predicted samples to 0\n",
    "    train_total = 0                 # Initialize the total number of samples to 0\n",
    "    total_batches = len(data_loader) # Get the total number of batches\n",
    "    train_progress_bar = tqdm(range(total_batches), desc=\"Training\", leave=False)             # Create a progress bar for training\n",
    "    for batch_num, (images, labels) in enumerate(data_loader):                                # Loop through the data loader\n",
    "        train_progress_bar.set_description(f\"Training Batch {batch_num+1}/{total_batches}\")   # Set the description of the progress bar\n",
    "        images, labels = images.to(device), labels.to(device)                                 # Move the images and labels to the device\n",
    "        optimizer.zero_grad()                                                               # Zero the gradients, as PyTorch automatically accumulates gradients\n",
    "        outputs = model(images)                                                           # Forward pass, get the model outputs\n",
    "        loss = criterion(outputs, labels)                                               # Calculate the loss\n",
    "        loss.backward()                                                                # Backward pass, calculate the gradients\n",
    "        optimizer.step()                                                            # Update the weights\n",
    "        running_loss += loss.item()                                                # Add the loss to the running loss\n",
    "        _, predicted = torch.max(outputs, 1)                                      # Get the predicted labels\n",
    "        train_total += labels.size(0)                                           # Add the number of samples in the batch to the total number of samples\n",
    "        train_correct += (predicted == labels).sum().item()                    # Add the number of correctly predicted samples to the total number of correctly predicted samples\n",
    "        train_progress_bar.update()                                          # Update the progress bar\n",
    "    train_progress_bar.close()                                            # Close the progress bar\n",
    "    return running_loss / len(data_loader), train_correct / train_total # Return the average loss and accuracy\n",
    "\n",
    "# Define the validation function\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0             # Initialize the running loss to 0\n",
    "    val_correct = 0               # Initialize the number of correctly predicted samples to 0\n",
    "    val_total = 0                # Initialize the total number of samples to 0\n",
    "    val_progress_bar = tqdm(data_loader, desc=\"Validating\", leave=False) # Create a progress bar for validation\n",
    "    with torch.no_grad():                                            # Disable gradient tracking\n",
    "        for images, labels in data_loader:                        # Loop through the data loader\n",
    "            val_progress_bar.set_description(\"Validating\")      # Set the description of the progress bar\n",
    "            images, labels = images.to(device), labels.to(device)  # Move the images and labels to the processing device\n",
    "            outputs = model(images)                             # Forward pass, get the model outputs\n",
    "            loss = criterion(outputs, labels)                 # Calculate the loss\n",
    "            running_loss += loss.item()                      # Add the loss to the running loss\n",
    "            _, predicted = torch.max(outputs, 1)           # Get the predicted labels \n",
    "            val_total += labels.size(0)                  # Add the number of samples in the batch to the total number of samples\n",
    "            val_correct += (predicted == labels).sum().item() # Add the number of correctly predicted samples to the total number of correctly predicted samples\n",
    "            val_progress_bar.update()                      # Update the progress bar \n",
    "    val_progress_bar.close()                              # Close the progress bar\n",
    "    return running_loss / len(data_loader), val_correct / val_total # Return the average loss and accuracy\n",
    "\n",
    "# Define the test function\n",
    "'''Currently, the test function is not being used in the training loop. \n",
    "However, you can use it to evaluate the model on the test dataset after training.'''\n",
    "def test(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_progress_bar = tqdm(data_loader, desc=\"Testing\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            test_progress_bar.set_description(\"Testing\")\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_progress_bar.update()\n",
    "    test_progress_bar.close()\n",
    "    return correct / total\n",
    "\n",
    "# Move the model to the device\n",
    "model.to(device) \n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Create the training dataset\n",
    "train_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
    "\n",
    "# Create the training data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the validation dataset\n",
    "val_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
    "\n",
    "# Create the validation data loader\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Train the model\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "    train_loss, train_accuracy = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "Let's take a look at how well we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix'\n",
    "        else:\n",
    "            title = 'Confusion Matrix, without Normalization'\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True Label',\n",
    "           xlabel='Predicted Label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt), ha='center', va='center', color='white' if cm[i, j] > thresh else 'black')\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "# Get the predictions for the validation dataset\n",
    "y_pred = []\n",
    "y_true = []\n",
    "val_progress_bar = tqdm(val_loader, desc=\"Predicting\", leave=False)\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "val_progress_bar.close()\n",
    "\n",
    "# Pull the class names from the dataset\n",
    "classes = val_dataset.classes\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plot_confusion_matrix(y_true, y_pred, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model was able to achieve a decent accuracy on the validation set, even with the unbalanced dataset. Does the model perform better or worse than the CNNs we trained in notebooks 1 and 1.2? Compare the results of running the three models for the same number of epochs and see how the ViT model stacks up against the CNNs.\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fair on some test images? Lets try some new images!\n",
    "1. Find your own image image.\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model on a few test images\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to the input size expected by ViT\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize as per ImageNet standards\n",
    "])\n",
    "\n",
    "# Define the test dataset\n",
    "test_dataset = torchvision.datasets.ImageFolder(data_path, transform=transform)\n",
    "\n",
    "# Create the test data loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Get the class names\n",
    "classes = test_dataset.classes\n",
    "\n",
    "# Function to run inference on a single image\n",
    "def run_inference(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = model(image.to(device))\n",
    "    \n",
    "    # Process the output\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    class_idx = predicted.item()\n",
    "\n",
    "    # Display the image and the prediction\n",
    "    plt.imshow(Image.open(image_path))\n",
    "    plt.title(f'Predicted Class: {classes[class_idx]}')  # Map index to class name\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Test the inference function\n",
    "test_image_path = 'data/bee_vs_wasp/bee/10007154554_026417cfd0_n.jpg'  # Update this with your test image path\n",
    "run_inference(test_image_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "How does this model compare to the CNNs you've used? Training time? Accuracy? What are some of the hyperparameters that are in ViT that aren't in CNNs?\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- Try different hyperparameters and see how they affect the model's performance.\n",
    "- Edit the model architecture to see how it affects the model's performance.\n",
    "- Attention Maps: Visualize the attention maps generated by the model to see which parts of the image the model is focusing on. To do this, you can modify the model to return the attention weights and visualize them using a heatmap. For more information, check out PyTorch's documentation on attention visualization: https://github.com/jacobgil/pytorch-grad-cam\n",
    "- Visualize the model architecture using TensorBoard or other visualization tools to better understand the model's structure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
