{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Computer Vision Tasks\n",
    "\n",
    "Now that Kevin has a better understanding of how computer vision classification works, he needs to learn more about other computer vision tasks. His manager has asked him to move on from wasps and bees to... fruits and (bounding) boxes! Kevin thankfully has an annotated dataset already, so he can start learning about object detection.\n",
    "\n",
    "As before, the dataset was found on Kaggle. [Check out the dataset information](https://www.kaggle.com/datasets/lakshaytyagi01/fruit-detection/data)\n",
    "\n",
    "<img src=\"images/fruits_detection_dataset-cover.jpg\" \n",
    "        alt=\"Image of fruits and bounding boxes from the dataset cover image\" \n",
    "        width=\"1000\" \n",
    "        height=\"600\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "## Pytorch\n",
    "\n",
    "This notebook will be a bit different than the previous ones: it doesn't use TensorFlow or Keras. Instead, it uses PyTorch, torchvision and YOLOv8. Whereas TensorFlow and Keras are easier to use for a variety of deep learning tasks (such as image classification), when it comes to object detection, PyTorch is currently more popular and better suited. YOLOv8 (You Only Look Once, version 8) is a popular object detection model that is known for its speed, accuracy and relative ease of implementation.\n",
    "\n",
    "As we've mentioned before, these tools are rapidly evolving, so it's important to stay up-to-date with the latest versions and best practices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook will be used for an Object Detection task that trains a model on the fruits_detection dataset using YOLOv8\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml \n",
    "from ultralytics import YOLO\n",
    "from ultralytics import settings\n",
    "\n",
    "import helpers_02\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "As we did in Notebook 1, we will have to find or download the dataset. This time the file is stored as a zip file, so we will need to extract it. \n",
    "\n",
    "You will also notice that instead of loading things to the **data** directory, the data is instead loaded to a new folder called **datasets**. This is a requirement of YOLOv8, which expects the data to be in a specific folder. The setting is *technically* possible to change, but is not something we want to hassle with right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to the helpers_01 functions in the prevoius notebooks,\n",
    "# The helpers_02.manage_data() function will check for and, if needed,\n",
    "# download the dataset.\n",
    "data_path = helpers_02.manage_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manage YOLO settings to set correct path to data folder\n",
    "settings.update({\"datasets_dir\": data_path})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains. We will also look at the annotations file, which contains the bounding box information for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a histogram of the number of images in each class\n",
    "def explore_data(data_path, show_picture=True, show_annotation=True, show_detection=True, show_hist=True):\n",
    "\n",
    "    # Define the class names\n",
    "    class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "    \n",
    "    # Examine some sample images\n",
    "    if show_picture:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))] \n",
    "\n",
    "        sample_images = []\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_path, folder, 'images', random.choice(os.listdir(os.path.join(data_path, folder, 'images'))))\n",
    "            sample_images.append(img_path)\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Examine the first five annotation files\n",
    "    if show_annotation:\n",
    "        annotation_files = []\n",
    "        for folder in os.listdir(data_path):\n",
    "            if os.path.isdir(os.path.join(data_path, folder)):\n",
    "                annotation_folder = os.path.join(data_path, folder, 'labels')\n",
    "                if os.path.exists(annotation_folder):\n",
    "                    for file in os.listdir(annotation_folder):\n",
    "                        annotation_files.append(os.path.join(annotation_folder, file))\n",
    "        for file in annotation_files[:5]:\n",
    "            with open(file, 'r') as f:\n",
    "                print(f\"File: {file}\")\n",
    "                for i, line in enumerate(f):\n",
    "                    if i > 4:\n",
    "                        break\n",
    "                    print(f\"  {line.strip()}\")\n",
    "\n",
    "    # Plot five random images with their associated, labeled bounding boxes\n",
    "    if show_detection:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))] \n",
    "\n",
    "        sample_images = []\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_path, folder, 'images', random.choice(os.listdir(os.path.join(data_path, folder, 'images'))))\n",
    "            annotation_path = os.path.join(data_path, folder, 'labels', os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "            sample_images.append((img_path, annotation_path))\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, (img_path, annotation_path) in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, x, y, w, h = map(float, line.strip().split())\n",
    "                    x, y, w, h = x * img.width, y * img.height, w * img.width, h * img.height\n",
    "                    rect = plt.Rectangle((x - w / 2, y - h / 2), w, h, fill=False, color='red', linewidth=2)\n",
    "                    axes[i].add_patch(rect)\n",
    "                    # Add class name above the bounding box\n",
    "                    axes[i].text(x - w / 2, y - h / 2, class_names[int(class_id)], color='red')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Make a histogram of the number of images in each class\n",
    "    if show_hist:\n",
    "        def get_class_counts(folder_path):  # Change from data_path to folder_path\n",
    "            class_counts = {}\n",
    "            labels_path = os.path.join(folder_path, 'labels')  # Add labels path\n",
    "            for filename in os.listdir(labels_path):  # Update listdir\n",
    "                with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id = int(line.split(' ')[0])  # Assuming labels are in YOLO format\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "            return class_counts\n",
    "\n",
    "        train_counts = get_class_counts(os.path.join(data_path, 'train'))  # Add os.path.join\n",
    "        val_counts = get_class_counts(os.path.join(data_path, 'valid'))\n",
    "        test_counts = get_class_counts(os.path.join(data_path, 'test'))\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        data_counts = {\n",
    "            'train': pd.Series(train_counts),\n",
    "            'val': pd.Series(val_counts),\n",
    "            'test': pd.Series(test_counts)\n",
    "        }\n",
    "        df = pd.DataFrame(data_counts)\n",
    "\n",
    "        df.plot.bar(figsize=(10, 6))\n",
    "        plt.xlabel('Class Name')\n",
    "        plt.xticks(np.arange(num_classes), class_names)\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.title('Distribution of Images per Class')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "explore_data(data_path, show_picture=True, show_annotation=True, show_detection=True, show_hist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot to unpack here! First, depending on the images sampled, we can see that the images are of different sizes and have different numbers of fruits. Other things to note:\n",
    "- Some of the images don't really contain fruits...\n",
    "- The file names are the same as the image names, but with a .txt extension.\n",
    "- The annotations file contains the class ID of the fruit (0 corresponds to 'Apple', etc.), and the bounding box coordinates. \n",
    "- The bounding box coordinates are in the format (x_min, y_min, x_max, y_max).\n",
    "- The bounding box coordinates are normalized, meaning that they are scaled to be between 0 and 1. This is a common practice in object detection tasks.\n",
    "- The bounding boxes in some of the images are not very accurate. This is a common problem in object detection tasks.\n",
    "- The dataset is very imbalanced, with a lot more oranges than other fruits.\n",
    "\n",
    "## 4. Create the YAML file\n",
    "YAML stands for \"YAML Ain't Markup Language\" and is a human-readable data serialization format. A YAML file is used to define the dataset configuration for training a YOLOv8 model. YAML configuration files are popular in deep learning because they are easier for humans to read and write, with the goal being to increase transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a YAML file for the YOLOv8 model configuration\n",
    "\n",
    "def create_yaml(data_path, class_names, yaml_file='fruits_detection_data.yaml'):\n",
    "    # Creates a YOLOv8 data.yaml file.\n",
    "    \n",
    "    yaml_dict = {\n",
    "        # 'path': data_path,  # Path to your dataset\n",
    "        'train': data_path + '/train/images',  # Relative path to training images\n",
    "        'val': data_path + '/valid/images',    # Relative path to validation images\n",
    "        'test': data_path + '/test/images',    # Relative path to testing images\n",
    "\n",
    "        'num_classes': len(class_names),   # Number of classes\n",
    "        'names': class_names      # List of class names\n",
    "    }\n",
    "\n",
    "    with open(yaml_file, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "    print(f'YAML file created: {yaml_file}')\n",
    "\n",
    "\n",
    "class_names = ['Apple', 'Banana', 'Grape', 'Orange', 'Pineapple', 'Watermelon']\n",
    "\n",
    "create_yaml(data_path, class_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and fit the model\n",
    "\n",
    "We will create a YOLOv8 model and fit it to the data. YOLOv8 has a lot of hyperparameters that can be tuned, but we will use the default values for now. For more information on the it's hyperparameters, [check out it's documentation](https://docs.ultralytics.com/modes/train/).\n",
    "\n",
    "Another neat feature of YOLOv8 is that by default it provides several evaluation metrics, such as the loss, precision, recall, and F1 score. This is very useful for monitoring the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the YOLOv8 model\n",
    "model = YOLO('yolov8n.yaml')\n",
    "results = model.train(data='fruits_detection_data.yaml', imgsz=640, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "Let's look at those evaluation metrics we mentioned above. YOLOv8 creates a **runs** folder that stores each training run. We'll pull them up here and examine what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "\n",
    "# Find the latest training run\n",
    "training = sorted(os.listdir('runs/detect/.'))\n",
    "latest_training = training[-1]\n",
    "print(f'Latest training run: {latest_training}')\n",
    "\n",
    "# Plot the .png files in the latest training run\n",
    "for file in os.listdir(f'runs/detect/{latest_training}'):\n",
    "    if file.endswith('.png'):\n",
    "        # Exclude the normalized confusion matrix since it's redundant\n",
    "        if 'normalized' in file:\n",
    "            continue\n",
    "        img = Image.open(f'runs/detect/{latest_training}/{file}')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the graphs\n",
    "If your stats are a little rusty like Kevin's, you might need a bit of refresher to make sense of the graphs above. Expand the section below for a rundown on the terms used above.\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary><h2>Click to Expand for Stats Terms!</h2></summary>\n",
    "\n",
    "<p>\n",
    "\n",
    "##### What is Precision?\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total predicted apples. Higher precision values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Recall?\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual class. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total actual apples. Higher recall values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Confidence?\n",
    "Confidence is the probability that a model assigns to a prediction. In our fruit object detection task, the confidence is the probability that a model assigns to a fruit being an apple, orange, or any other fruit. Higher confidence values are better, as they indicate that the model is more certain about its predictions.\n",
    "\n",
    "##### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "##### What is the F1 Confidence Curve?\n",
    "The F1 Confidence Curve is a plot of the F1 score against the confidence threshold. The F1 score is the harmonic mean of precision and recall, and it is a measure of a model's accuracy. The confidence threshold is the minimum confidence level that a model must have in order to make a prediction. The F1 Confidence Curve is used to determine the optimal confidence threshold for a model. The curve shows how the F1 score changes as the confidence threshold is varied. The goal is to find the confidence threshold that maximizes the F1 score. The highest point on the curve tells you the optimal confidence threshold where the model strikes the best balance between precision and recall. In our fruit object detection task, the F1 Confidence Curve would show how the F1 score changes as the confidence threshold is varied for each fruit class. Higher F1 scores are better, as they indicate that the model is making more accurate predictions.\n",
    "    \n",
    "##### What is mAP50?\n",
    "mAP50 stands for \"mean Average Precision at 50% confidence\". It is a common metric used in object detection tasks to evaluate the performance of a model. The mAP50 is the average of the precision values at different recall levels, where the recall is calculated at a confidence threshold of 50%. A higher mAP50 value indicates better performance.\n",
    "    \n",
    "##### What is mAP50-95?\n",
    "mAP50-95 is the mean average precision (mAP) over the range of intersection over union (IoU) thresholds from 0.5 to 0.95. IoU is a measure of the overlap between two bounding boxes. It is calculated as the area of the intersection of the two bounding boxes divided by the area of their union. An IoU of 0 means that the two bounding boxes do not overlap at all, while an IoU of 1 means that the two bounding boxes are identical. The mAP50-95 metric is a more comprehensive metric than the mAP50 metric, which only considers the IoU threshold of 0.5. The mAP50-95 metric provides a more complete picture of the model's performance across a range of IoU thresholds.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "In addition to those graphs, we can look at the predictions themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the labels and predictions from the last training run\n",
    "\n",
    "# Load the images\n",
    "img1 = Image.open(f'runs/detect/{latest_training}/val_batch0_labels.jpg')\n",
    "img2 = Image.open(f'runs/detect/{latest_training}/val_batch0_pred.jpg')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the prediction images, you'll probably notice this model loves its citrus. That's no surprise given the histogram we looked at at the beginning of this notebook. What could we do to improve this?\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fair on some test images? After you run the cell below:\n",
    "1. Find your own image of fruit.\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the model on a few test images\n",
    "\n",
    "# Get ten random test images\n",
    "test_images = []\n",
    "for folder in os.listdir(os.path.join(data_path, 'test', 'images')):\n",
    "    img_path = os.path.join(data_path, 'test', 'images', folder)\n",
    "    test_images.append(img_path)\n",
    "test_images = random.sample(test_images, 10)\n",
    "\n",
    "# Plot the test images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i // 5, i % 5].imshow(img)\n",
    "    axes[i // 5, i % 5].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Run the model on the test images\n",
    "infer_results = model.predict(test_images)\n",
    "\n",
    "for img_path, result in zip(test_images, infer_results):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((640, 640))\n",
    "\n",
    "    # Create the plot for this image\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Access bounding boxes and class information\n",
    "    boxes = result.boxes\n",
    "    class_ids = boxes.cls.cpu().numpy()  # Class IDs are on the CPU\n",
    "    confidences = boxes.conf.cpu().numpy()  # Confidences are on the CPU\n",
    "\n",
    "    for box, class_id, conf in zip(boxes.xyxy, class_ids, confidences):  \n",
    "        # boxes.xyxy are in [xmin, ymin, xmax, ymax] format\n",
    "        x1, y1, x2, y2 = box.cpu()  # Move coordinates to CPU\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        rect = plt.Rectangle((x1, y1), w, h, fill=False, color='red', linewidth=2)\n",
    "        plt.gca().add_patch(rect)\n",
    "        plt.text(x1, y1, result.names[int(class_id)], color='red')  \n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "A good first place to start would be YOLOv8's documentation so you can understand what hyperparameters you have access to and how changing them will affect training. Make some adjustments and see how high you can get your fruits object detection F1 score!\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- You might have noticed the *yolov8n.pt* file that is added to the folder when you load YOLO. That is the pre-trained model using the [COCO dataset](https://cocodataset.org/#home). The YOLOv8 documentation linked above provides instructions for transer learning and fine-tuning with the pre-trained model. Give it a shot!\n",
    "\n",
    "- If you're feeling very bold and want the full Object Detection Experience, find an image labeler and build your own dataset! If you use proprietary data, be sure that the software you use to label meets your institution's guidelines for data sharing. If you go this route, you will almost certainly want to use the fine-tuning option mentioned above, otherwise you're going to be spending quite a long time labeling..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolo-v8.2",
   "language": "python",
   "name": "yolo-v8.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
