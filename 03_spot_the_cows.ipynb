{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Computer Vision Models\n",
    "\n",
    "!!! Flavor text goes here!!!\n",
    "\n",
    "As before, the dataset was found on Kaggle. [Check out the dataset information](https://www.kaggle.com/datasets/amiteshpatra07/cattle-dataset-pig-sheep-cow-horse)\n",
    "\n",
    "<img src=\"images/cattle_segmentation_cover.png\" \n",
    "        alt=\"Image of a cow in a field.\" \n",
    "        width=\"1000\" \n",
    "        height=\"200\" \n",
    "        style=\"display: block; margin: 0 auto\" />\n",
    "\n",
    "!!! Insert Notes Here !!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook will be used for an Object Detection task that trains a model on the cattle_segmentation dataset using YOLOv8\n",
    "\n",
    "# Importing the necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import pathlib\n",
    "import requests\n",
    "import zipfile\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import yaml \n",
    "from ultralytics import YOLO\n",
    "from ultralytics import settings\n",
    "\n",
    "import helpers_02\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Training on {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting the data\n",
    "\n",
    "Like Notebooks 1 and 2, we will have to find or download the dataset. This time the file is stored as a zip file, so we will need to extract it. \n",
    "\n",
    "You will also notice that instead of loading things to the **data** directory, the data is instead loaded to a new folder called **datasets**. This is a requirement of YOLOv8, which expects the data to be in a specific folder. The setting is *technically* possible to change, but is not something we want to hassle with right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Similar to the helpers_01 functions in the prevoius notebooks,\n",
    "# The helpers_02.manage_data() function will check for and, if needed,\n",
    "# download the dataset.\n",
    "data_path = helpers_02.manage_data(url=\"https://www.dropbox.com/scl/fi/6r8z3gmuy9plji8pd8nti/cattle_segmentation.zip?rlkey=uekp4dlikoobhapvw3vsjq0d4&st=6soxt9rk&dl=1\", \n",
    "                                   filename=\"cattle_segmentation.zip\",\n",
    "                                  folder_name='cattle_segmentation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Manage YOLO settings to set correct path to data folder\n",
    "settings.update({\"datasets_dir\": data_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the dataset\n",
    "\n",
    "We will take a look at the dataset to see what it contains. We will also look at the annotations file, which contains the bounding box information for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign the path to the dataset\n",
    "data_dir = r\"datasets/cattle_segmentation\"\n",
    "\n",
    "# Make a histogram of the number of images in each class\n",
    "def explore_data(data_path, show_picture=True, show_annotation=True, show_segmentation=True, show_hist=True):\n",
    "\n",
    "    # Define the class names\n",
    "    class_names = ['Cow', 'Horse', 'Pig', 'Sheep', 'Undefined']\n",
    "\n",
    "    # Initialize sample images list to store paths\n",
    "    sample_images = []\n",
    "\n",
    "    # Examine some sample images\n",
    "    if show_picture:\n",
    "        # Get valid image folders \n",
    "        image_folders = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))] \n",
    "\n",
    "        for i in range(5):\n",
    "            folder = random.choice(image_folders) \n",
    "            img_path = os.path.join(data_path, folder, 'images', random.choice(os.listdir(os.path.join(data_path, folder, 'images'))))\n",
    "            sample_images.append(img_path)\n",
    "\n",
    "        # Plot the sample images\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, img_path in enumerate(sample_images):\n",
    "            img = Image.open(img_path)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    # Examine the first annotation file\n",
    "    if show_annotation:\n",
    "        annotation_files = []\n",
    "        for folder in os.listdir(data_path):\n",
    "            if os.path.isdir(os.path.join(data_path, folder)):\n",
    "                annotation_folder = os.path.join(data_path, folder, 'labels')\n",
    "                if os.path.exists(annotation_folder):\n",
    "                    for file in os.listdir(annotation_folder):\n",
    "                        annotation_files.append(os.path.join(annotation_folder, file))\n",
    "        if annotation_files:\n",
    "            annotation_file = annotation_files[0]  # Show only the first annotation file\n",
    "            with open(annotation_file, 'r') as f:\n",
    "                print(f\"File: {annotation_file}\")\n",
    "                for i, line in enumerate(f):\n",
    "                    if i > 4:\n",
    "                        break\n",
    "                    print(f\"  {line.strip()}\")\n",
    "\n",
    "    # Plot the same sample images with their associated, labeled segmentation masks\n",
    "    if show_segmentation and sample_images:\n",
    "        # Find corresponding annotation files for the sample images\n",
    "        sample_annotations = []\n",
    "        for img_path in sample_images:\n",
    "            folder = os.path.dirname(os.path.dirname(img_path))\n",
    "            annotation_path = os.path.join(folder, 'labels', os.path.basename(img_path).replace('.jpg', '.txt'))\n",
    "            sample_annotations.append((img_path, annotation_path))\n",
    "\n",
    "        # Plot the sample images with segmentation masks\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n",
    "        for i, (img_path, annotation_path) in enumerate(sample_annotations):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            with open(annotation_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    class_id, *polygon = map(float, line.strip().split())\n",
    "                    polygon = np.array(polygon).reshape(-1, 2) * [img.shape[1], img.shape[0]]\n",
    "                    polygon = polygon.astype(np.int32)\n",
    "                    cv2.fillPoly(mask, [polygon], 255)\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].imshow(mask, alpha=0.5, cmap='jet')\n",
    "            axes[i].axis('off')\n",
    "            # Add class name above the segmentation mask\n",
    "            if len(polygon) > 0:\n",
    "                x, y = polygon[0]\n",
    "                axes[i].text(x, y, class_names[int(class_id)], color='white', backgroundcolor='red')\n",
    "        plt.show()\n",
    "\n",
    "    # Make a histogram of the number of images in each class\n",
    "    if show_hist:\n",
    "        def get_class_counts(folder_path):  # Change from data_path to folder_path\n",
    "            class_counts = {}\n",
    "            labels_path = os.path.join(folder_path, 'labels')  # Add labels path\n",
    "            for filename in os.listdir(labels_path):  # Update listdir\n",
    "                with open(os.path.join(labels_path, filename), 'r') as f:\n",
    "                    for line in f:\n",
    "                        class_id = int(line.split(' ')[0])  # Assuming labels are in YOLO format\n",
    "                        class_counts[class_id] = class_counts.get(class_id, 0) + 1\n",
    "            return class_counts\n",
    "\n",
    "        train_counts = get_class_counts(os.path.join(data_path, 'train'))  # Add os.path.join\n",
    "        val_counts = get_class_counts(os.path.join(data_path, 'valid'))\n",
    "        test_counts = get_class_counts(os.path.join(data_path, 'test'))\n",
    "        num_classes = len(class_names)\n",
    "\n",
    "        data_counts = {\n",
    "            'train': pd.Series(train_counts),\n",
    "            'val': pd.Series(val_counts),\n",
    "            'test': pd.Series(test_counts)\n",
    "        }\n",
    "        df = pd.DataFrame(data_counts)\n",
    "\n",
    "        df.plot.bar(figsize=(10, 6))\n",
    "        plt.xlabel('Class Name')\n",
    "        plt.xticks(np.arange(num_classes), class_names)\n",
    "        plt.ylabel('Number of Images')\n",
    "        plt.title('Distribution of Images per Class')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "explore_data(data_path, show_picture=True, show_annotation=True, show_segmentation=True, show_hist=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the fruit detection notebook, there are some issues with this cattle dataset!\n",
    "- Not all the animals in some of the images are annotated.\n",
    "- The annotation file names are the same as the image names, but with a .txt extension.\n",
    "- The annotations file contains the class ID of the animal (0 corresponds to 'Cow', etc.), and the segmentation coordinates. \n",
    "- The segmenation coordinates are the pixel locations for the vertices of the segmenting polygons.\n",
    "- The segmentation coordinates are normalized, meaning that they are scaled to be between 0 and 1. This is a common practice in object detection tasks.\n",
    "- The dataset is very imbalanced, with a lot more pigs than other animals, and comparitively almost no horses.\n",
    "\n",
    "## 4. Create the YAML file\n",
    "YAML stands for \"YAML Ain't Markup Language\" and is a human-readable data serialization format. A YAML file is used to define the dataset configuration for training a YOLOv8 model. YAML configuration files are popular in deep learning because they are easier for humans to read and write, with the goal being to increase transparency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a YAML file for the YOLOv8 model configuration\n",
    "\n",
    "def create_yaml(data_path, class_names, yaml_file='cattle_segmentation_data.yaml'):\n",
    "    # Creates a YOLOv8 data.yaml file.\n",
    "    \n",
    "    yaml_dict = {\n",
    "        # 'path': data_path,  # Path to your dataset\n",
    "        'train': data_path + '/train/images',  # Relative path to training images\n",
    "        'val': data_path + '/valid/images',    # Relative path to validation images\n",
    "        'test': data_path + '/test/images',    # Relative path to testing images\n",
    "\n",
    "        'num_classes': len(class_names),   # Number of classes\n",
    "        'names': class_names      # List of class names\n",
    "    }\n",
    "\n",
    "    with open(yaml_file, 'w') as outfile:\n",
    "        yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "    print(f'YAML file created: {yaml_file}')\n",
    "\n",
    "data_dir = 'cattle_segmentation'\n",
    "class_names = ['Cow', 'Horse', 'Pig', 'Sheep', 'Undefined']\n",
    "\n",
    "create_yaml(data_path, class_names) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create and fit the model\n",
    "\n",
    "We will create a YOLOv8 model and fit it to the data. YOLOv8 has a lot of hyperparameters that can be tuned, but we will use the default values for now. For more information on the it's hyperparameters, [check out it's documentation](https://docs.ultralytics.com/modes/train/).\n",
    "\n",
    "Another neat feature of YOLOv8 is that by default it provides several evaluation metrics, such as the loss, precision, recall, and F1 score. This is very useful for monitoring the model's performance during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the YOLOv8 model\n",
    "model = YOLO('yolov8n-seg.yaml')\n",
    "results = model.train(data='cattle_segmentation_data.yaml', imgsz=640, epochs=10, show_boxes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the results\n",
    "\n",
    "Let's look at those evaluation metrics we mentioned above. YOLOv8 creates a **runs** folder that stores each training run. We'll pull them up here and examine what they mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the evaluation results\n",
    "\n",
    "# Find the latest training run\n",
    "training = sorted(os.listdir('runs/segment/.'))\n",
    "latest_training = training[-1]\n",
    "print(f'Latest training run: {latest_training}')\n",
    "\n",
    "# Plot the .png files in the latest training run\n",
    "for file in os.listdir(f'runs/segment/{latest_training}'):\n",
    "    if file.endswith('.png'):\n",
    "        # Exclude the normalized confusion matrix since it's redundant\n",
    "        if 'normalized' in file:\n",
    "            continue\n",
    "        img = Image.open(f'runs/segment/{latest_training}/{file}')\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down the graphs\n",
    "In case you need more help with the metrics above, here is a little primer. Click the arrow to expand the section below for a rundown on the terms used above.\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary><h2>Click to Expand for Stats Terms!</h2></summary>\n",
    "\n",
    "<p>\n",
    "\n",
    "##### What is Precision?\n",
    "Precision is the ratio of correctly predicted positive observations to the total predicted positives. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total predicted apples. Higher precision values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Recall?\n",
    "Recall is the ratio of correctly predicted positive observations to the all observations in actual class. An example in our fruit object detection task would be the ratio of correctly predicted apples to the total actual apples. Higher recall values are better, as they indicate that the model is making more accurate predictions.\n",
    "\n",
    "##### What is Confidence?\n",
    "Confidence is the probability that a model assigns to a prediction. In our fruit object detection task, the confidence is the probability that a model assigns to a fruit being an apple, orange, or any other fruit. Higher confidence values are better, as they indicate that the model is more certain about its predictions.\n",
    "\n",
    "##### What is a confusion matrix?\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm. The confusion matrix shows the ways in which your classification model is confused when it makes predictions. It gives you insight not only into the errors being made by your classifier but more importantly the types of errors that are being made.\n",
    "\n",
    "##### What is the F1 Confidence Curve?\n",
    "The F1 Confidence Curve is a plot of the F1 score against the confidence threshold. The F1 score is the harmonic mean of precision and recall, and it is a measure of a model's accuracy. The confidence threshold is the minimum confidence level that a model must have in order to make a prediction. The F1 Confidence Curve is used to determine the optimal confidence threshold for a model. The curve shows how the F1 score changes as the confidence threshold is varied. The goal is to find the confidence threshold that maximizes the F1 score. The highest point on the curve tells you the optimal confidence threshold where the model strikes the best balance between precision and recall. In our fruit object detection task, the F1 Confidence Curve would show how the F1 score changes as the confidence threshold is varied for each fruit class. Higher F1 scores are better, as they indicate that the model is making more accurate predictions.\n",
    "    \n",
    "##### What is mAP50?\n",
    "mAP50 stands for \"mean Average Precision at 50% confidence\". It is a common metric used in object detection tasks to evaluate the performance of a model. The mAP50 is the average of the precision values at different recall levels, where the recall is calculated at a confidence threshold of 50%. A higher mAP50 value indicates better performance.\n",
    "    \n",
    "##### What is mAP50-95?\n",
    "mAP50-95 is the mean average precision (mAP) over the range of intersection over union (IoU) thresholds from 0.5 to 0.95. IoU is a measure of the overlap between two bounding boxes. It is calculated as the area of the intersection of the two bounding boxes divided by the area of their union. An IoU of 0 means that the two bounding boxes do not overlap at all, while an IoU of 1 means that the two bounding boxes are identical. The mAP50-95 metric is a more comprehensive metric than the mAP50 metric, which only considers the IoU threshold of 0.5. The mAP50-95 metric provides a more complete picture of the model's performance across a range of IoU thresholds.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "In addition to those graphs, we can look at the predictions themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the labels and predictions from the last training run\n",
    "\n",
    "# Load the images\n",
    "img1 = Image.open(f'runs/segment/{latest_training}/val_batch2_labels.jpg')\n",
    "img2 = Image.open(f'runs/segment/{latest_training}/val_batch2_pred.jpg')\n",
    "\n",
    "# Plot the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
    "axes[0].imshow(img1)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Ground Truth')\n",
    "axes[1].imshow(img2)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Predictions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions above aren't perfect, but they are pretty good! The model is able to detect the animals in the images and put segmentation masks over them. For the most part, the segmentation masks hold up to the animals's shapes, but it doesn't always predict the right class. For example, the model sometimes predicts a cow as a pig, or a horse as a sheep. What are some ways we could improve the model's performance?\n",
    "\n",
    "## 7. Inference\n",
    "How does the model fair on some test images? After you run the cell below:\n",
    "1. Find your own image of a cow, pig, sheep, or horse (or any other animal you like).\n",
    "2. Upload it to this folder.\n",
    "3. Add or edit the code below to run on the new image rather than images in the test folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the dataset path\n",
    "data_dir = r\"datasets/cattle_segmentation\"\n",
    "\n",
    "# Get ten random test images\n",
    "test_images = []\n",
    "for img_name in os.listdir(os.path.join(data_dir, 'test', 'images')):\n",
    "    img_path = os.path.join(data_dir, 'test', 'images', img_name)\n",
    "    test_images.append(img_path)\n",
    "test_images = random.sample(test_images, 10)\n",
    "\n",
    "# Plot the test images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "for i, img_path in enumerate(test_images):\n",
    "    img = Image.open(img_path)\n",
    "    axes[i // 5, i % 5].imshow(img)\n",
    "    axes[i // 5, i % 5].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Run the model on the test images\n",
    "infer_results = model(test_images)\n",
    "\n",
    "for img_path, result in zip(test_images, infer_results):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize((640, 640))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Check if segmentation masks are available\n",
    "    if result.masks is not None:\n",
    "        # Access segmentation masks and class information\n",
    "        masks = result.masks.data.cpu().numpy()  # Masks are on the CPU\n",
    "        class_ids = result.boxes.cls.cpu().numpy()  # Class IDs are on the CPU\n",
    "        confidences = result.boxes.conf.cpu().numpy()  # Confidences are on the CPU\n",
    "\n",
    "        for mask, class_id, conf in zip(masks, class_ids, confidences):\n",
    "            # Create a mask overlay\n",
    "            mask = mask.squeeze()\n",
    "            plt.imshow(mask, alpha=0.05, cmap='spring')\n",
    "\n",
    "            # Get the contour of the mask\n",
    "            contours, _ = cv2.findContours(mask.astype(np.uint8), cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            for contour in contours:\n",
    "                plt.plot(contour[:, 0, 0], contour[:, 0, 1], color='red', linewidth=2)\n",
    "            \n",
    "            # Display class name and confidence score for each instance\n",
    "            mask_center_x = int(mask.shape[1] / 2)\n",
    "            mask_center_y = int(mask.shape[0] / 2)\n",
    "            plt.text(mask_center_x, mask_center_y, f\"{result.names[int(class_id)]} {conf:.2f}\", color='white', backgroundcolor='red')\n",
    "    else:\n",
    "        plt.text(320, 320, \"No class detected\", fontsize=12, color='white', backgroundcolor='red', ha='center')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Explore hyperparameters\n",
    "\n",
    "Now that you have a good baseline, consider how you might deal with this model's issues.\n",
    "- How would you address issues in the dataset?\n",
    "- How would you optimize training?\n",
    "\n",
    "A good first place to start would be YOLOv8's documentation so you can understand what hyperparameters you have access to and how changing them will affect training. Make some adjustments and see how high you can get your cattle segmentation F1 score!\n",
    "\n",
    "## Bonus Exercises\n",
    "\n",
    "- You might have noticed the *yolov8n-seg.pt* file that is added to the folder when you load YOLO. That is the pre-trained model using the [COCO dataset](https://cocodataset.org/#home). The YOLOv8 documentation linked above provides instructions for transfer learning and fine-tuning with the pre-trained model. Give it a shot!\n",
    "\n",
    "- The dataset is imbalanced, with a lot more pigs than other animals. How would you address this issue? (Image augmentation is a little more difficult with segmentation tasks, but there are still some techniques you could try. Check out the [albumentations library](https://albumentations.ai/docs/getting_started/image_augmentation/).)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolo-v8.2",
   "language": "python",
   "name": "yolo-v8.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
