{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src='https://github.com/PracticumAI/deep_learning/blob/main/images/practicumai_deep_learning.png?raw=true' alt='Practicum AI: Deep Learning Foundations icon' align='right' width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Class Imbalance in Data\n",
    "\n",
    "Kevin now understands CNNs and how they work, but his model is still not performing as well as he would like. He's been reading up on some techniques to improve his model's performance and is excited to try them out. \n",
    "\n",
    "In this notebook, we will explore two methods to help mitigate the issues that class imbalance poses: stratification and class weighting.\n",
    "\n",
    "* **Stratification:** Ensures that class frequencies are maintained when making the training and validation datasets. In extreme cases, minority classes can be totally absent from a sampled dataset. But even in the case of less extreme imbalances and larger datasets, stratification can ensure class distributions are the same and help the model train.\n",
    "\n",
    "* **Class weighting:** By default, the probability of any class is assumed to be equal when training a model. In the case of imbalanced classes, that can cause an issue. We can calculate per-class probabilities and use the class weights when training the model.\n",
    "\n",
    "\n",
    "One particularly good article on this is topic is Ayush Thakur and Aritra Roy Gosthipaty's article [Simple Ways to Tackle Class Imbalance](https://wandb.ai/authors/class-imbalance/reports/Simple-Ways-to-Tackle-Class-Imbalance--VmlldzoxODA3NTk) (Thakur & Gosthipaty). While we won't cover all the methods outlined, it's a good resource.\n",
    "\n",
    "## A note about class imbalance and the bees vs wasp dataset\n",
    "\n",
    "One important point out is that class imbalance can arise from several causes. In the case with our bee and wasp data, the fundamental issue is more of a data quality and collection issue. The real answer to solving the problem has more to do with rethinking the data:\n",
    "\n",
    "* Do the categories rally make sense? Probably not--the non-insect category is not really helpful in this case and probably causes more issues than it solves. \n",
    "* Can better data solve the problem? Most likely--we can get more images of bees and other insects.\n",
    "* Are the images quality images? In many cases, not--many images would be impossible for a human to identify! We must trust the labels.\n",
    "\n",
    "Data quality is critical, though not something we will tackle here. Our hyperparameter tuning, stratification, and using class weights will only do so much to improve the results.\n",
    "\n",
    "But, in many cases, class imbalance is an intrinsic part of the data. Rare diseases are a classic example of this. If a disease has an incidence of only 1%, training data for that class will be limited. In these cases, the tools we will cover may be more critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the libraries we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf   # Import the TensorFlow library, which provides tools for deep learning.\n",
    "import pandas as pd  # Import the pandas library, used for data manipulation and analysis.\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime\n",
    "\n",
    "# Used for data management\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import tarfile\n",
    "\n",
    "import matplotlib.pyplot as plt  # Import the matplotlib library for plotting and visualization.\n",
    "# This line allows for the display of plots directly within the Jupyter notebook interface.\n",
    "%matplotlib inline  \n",
    " \n",
    "# Import Keras libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Import the ImageDataGenerator class from Keras module in TensorFlow.\n",
    "from tensorflow.keras.models import Sequential  # Import the Sequential model: a linear stack of layers from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Dense  # Import the Dense layer: a fully connected neural network layer from Keras module in TensorFlow.\n",
    "from tensorflow.keras.layers import Flatten  # Import the Flatten layer: used to convert input data into a 1D array from Keras module in TensorFlow.\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy  # Import the SparseCategoricalCrossentropy loss function from Keras module in TensorFlow.\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras import losses\n",
    "from sklearn.metrics import confusion_matrix \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Import helper functions--most of these were in the cells of the DLF_03_bees_vs_wasps.ipynb notebook.\n",
    "# We moved them out of the notebook to enhace readability. \n",
    "import helpers_01, helpers_01_2\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the data\n",
    "\n",
    "As before we'll use our helper function `helpers_01.manage_data` to make sure we have the data and set the `data_path`.\n",
    "\n",
    "In the last notebook, Kevin noticed that his model had a big problem: it thought everything was wasps! Let's explore the data a bit and see if we can figure out why.\n",
    "\n",
    "First let's make a histogram of the number of images in each class. That should give us a good idea of how balanced the dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Chech for the data.\n",
    "data_path = helpers_01.manage_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make a histogram of the number of images in each class\n",
    "def make_hist_get_max(data_path):\n",
    "    # Get the class names\n",
    "    class_names = os.listdir(data_path)\n",
    "\n",
    "    # Initialize an empty list to store the number of images in each class\n",
    "    num_images = []\n",
    "\n",
    "    # Loop through each class\n",
    "    for class_name in class_names:\n",
    "        # Get the list of images in the class\n",
    "        images = os.listdir(os.path.join(data_path, class_name))\n",
    "        # Append the number of images in the class to the list\n",
    "        num_images.append(len(images))\n",
    "\n",
    "    # Put the number of images in each class in descending order\n",
    "    num_images, class_names = zip(*sorted(zip(num_images, class_names), reverse=True))\n",
    "\n",
    "    total_images = sum(num_images)\n",
    "    \n",
    "    # Print the number of images in each class\n",
    "    for i in range(len(class_names)):\n",
    "        percentage = (num_images[i] / total_images) * 100\n",
    "        print(f'Category {class_names[i]}: {num_images[i]:,} images, or {percentage:.1f}% of total images.')\n",
    "\n",
    "    print(\"************************************\")       \n",
    "        \n",
    "    # Create a histogram of the number of images in each class\n",
    "    plt.bar(class_names, num_images)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.title('Number of Images in Each Class')\n",
    "    plt.show()\n",
    "\n",
    "    return num_images\n",
    "\n",
    "num_images = make_hist_get_max(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes! That's a pretty big class imbalance--there are almost 5,000 wasp images and less than 1,000 other non-insect images. Large class imbalances tend to make models biased towards the majority class. This is likely why Kevin's model was predicting everything as wasps. Let's try to fix this!\n",
    "\n",
    "Part of the issue with imballanced data is that the algorithm can do fairly well by never predicting the rare class(es).\n",
    "\n",
    "As an example, let's imaging we are looking at a rare disease. If only 5% of the data are from the disease state, a model can be 95% accurate by never predicting disease. That 95% accuracy sounds good, but the model is not too helpful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stratification\n",
    "\n",
    "The first method we will implement is stratification. Again, stratification ensures that when our dataset is split into training and validation sets, the class frequencies match the frequencies of the original data. This may not seem like a big issue with our dataset, but as we will see, it does help a lot! Ensuring that training and validation class frequencies are the same is important.\n",
    "\n",
    "Let's try this out. We'll use the `train_test_split` function from `sklearn` to split the data into training and validation sets. We'll set the `stratify` parameter to the labels so that the function samples in a stratified manner.\n",
    "\n",
    "We'll use a slightly modified version of the `load_display_data` function we used in the first notebook. It is now in the [`helpers_01`](helpers_01.py) module we imported. We will pass `show_pictures=False` to skip that.\n",
    "\n",
    "You can look in the [`helpers_01`](helpers_01.py) file for the full code, but the code for stratifying is here:\n",
    "\n",
    "```python\n",
    " if stratify: # Use sklearn's train_test_split function to split the data \n",
    "                 # into training and testing sets\n",
    "        # Split the data in a stratified manner\n",
    "        X_train, X_val, y_train, y_val = train_test_split(images, labels, \n",
    "                                test_size=0.2, stratify=labels)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data, this updated function changes the code to accomodate stratification\n",
    "# and using class weights. Checkout the file if you want.\n",
    "X_train, X_valid = helpers_01_2.load_display_data(data_path, batch_size=32, shape=(80,80,3), show_pictures=False, stratify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the same model we had at the end of notebook 01.1_bees_vs_wasps.ipynb\n",
    "# The model summary is shown, but model definition code is now moved to the \n",
    "# helpers_01_2.py file.\n",
    "\n",
    "model = helpers_01_2.make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compile and train the model\n",
    "# Due to change in dataset format, we change to categorical_correntropy loss. \n",
    "# This is the same loss function except in the format of the labels\n",
    "\n",
    "model, history = helpers_01_2.compile_train_model(X_train, X_valid, model, \n",
    "                        log_name='stratify', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "helpers_01_2.evaluate_model(X_train, X_valid, model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Weighting\n",
    "\n",
    "The next method we will implement is adding class weighting. As Thakur & Gosthipaty write:\n",
    "> One of the easiest ways to counter class imbalance is to use class weights wherein we give different weightage to different classes. The number of samples in the classes is considered while computing the class weights. We apply more significant weight to a minority class, which places more emphasis on that class. The classifier thus learns equally from both the classes. [Their example had only two classes]\n",
    ">\n",
    "> Class weights regularize the loss function. By misclassifying the minority class, a higher loss is incurred by the model since the minority class has a higher weight. This forces the model to learn representations for the minority class. This, however, comes at a price of slightly reduced performance for the majority class.\n",
    "\n",
    "Note that they were dealing with two classes, here, we have four. Our code will handle datasets with different numbers of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, cls_counts = helpers_01_2.load_display_data(data_path, \n",
    "                batch_size=32, shape=(80,80,3), show_pictures=False, \n",
    "                stratify=True, return_cls_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Make our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make the model\n",
    "\n",
    "model = helpers_01_2.make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compile and Train the model\n",
    "\n",
    "This time we will have both stratification and class weighting turned on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "model, history = helpers_01_2.compile_train_model(X_train, X_valid, model,\n",
    "                        cls_counts=cls_counts, log_name='weights',\n",
    "                        loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 7. Evaluate the model\n",
    "\n",
    "Now that we have trained our model let's evaluate how it does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "helpers_01_2.evaluate_model(X_train, X_valid, model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.15",
   "language": "python",
   "name": "tensorflow-2.15"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
